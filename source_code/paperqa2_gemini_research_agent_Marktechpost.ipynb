{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install paper-qa>=5 google-generativeai requests pypdf2 -q\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "import tempfile\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from paperqa import Settings, ask, agent_query\n",
        "from paperqa.settings import AgentSettings\n",
        "import google.generativeai as genai\n",
        "\n",
        "GEMINI_API_KEY = \"Use Your Own API Key Here\"\n",
        "os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
        "\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "print(\"✅ Gemini API key configured successfully!\")"
      ],
      "metadata": {
        "id": "4RY2OO-RQGGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_sample_papers():\n",
        "    \"\"\"Download sample AI/ML research papers for demonstration\"\"\"\n",
        "    papers = {\n",
        "        \"attention_is_all_you_need.pdf\": \"https://arxiv.org/pdf/1706.03762.pdf\",\n",
        "        \"bert_paper.pdf\": \"https://arxiv.org/pdf/1810.04805.pdf\",\n",
        "        \"gpt3_paper.pdf\": \"https://arxiv.org/pdf/2005.14165.pdf\"\n",
        "    }\n",
        "\n",
        "    papers_dir = Path(\"sample_papers\")\n",
        "    papers_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    print(\"📥 Downloading sample research papers...\")\n",
        "    for filename, url in papers.items():\n",
        "        filepath = papers_dir / filename\n",
        "        if not filepath.exists():\n",
        "            try:\n",
        "                response = requests.get(url, stream=True, timeout=30)\n",
        "                response.raise_for_status()\n",
        "                with open(filepath, 'wb') as f:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "                print(f\"✅ Downloaded: {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Failed to download {filename}: {e}\")\n",
        "        else:\n",
        "            print(f\"📄 Already exists: {filename}\")\n",
        "\n",
        "    return str(papers_dir)\n",
        "\n",
        "papers_directory = download_sample_papers()\n",
        "\n",
        "def create_gemini_settings(paper_dir: str, temperature: float = 0.1):\n",
        "    \"\"\"Create optimized settings for PaperQA2 with Gemini models\"\"\"\n",
        "\n",
        "    return Settings(\n",
        "        llm=\"gemini/gemini-1.5-flash\",\n",
        "        summary_llm=\"gemini/gemini-1.5-flash\",\n",
        "\n",
        "        agent=AgentSettings(\n",
        "            agent_llm=\"gemini/gemini-1.5-flash\",\n",
        "            search_count=6,\n",
        "            timeout=300.0,\n",
        "        ),\n",
        "\n",
        "        embedding=\"gemini/text-embedding-004\",\n",
        "\n",
        "        temperature=temperature,\n",
        "        paper_directory=paper_dir,\n",
        "\n",
        "        answer=dict(\n",
        "            evidence_k=8,\n",
        "            answer_max_sources=4,\n",
        "            evidence_summary_length=\"about 80 words\",\n",
        "            answer_length=\"about 150 words, but can be longer\",\n",
        "            max_concurrent_requests=2,\n",
        "        ),\n",
        "\n",
        "        parsing=dict(\n",
        "            chunk_size=4000,\n",
        "            overlap=200,\n",
        "        ),\n",
        "\n",
        "        verbosity=1,\n",
        "    )"
      ],
      "metadata": {
        "id": "soMy6Ap5QR2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PaperQAAgent:\n",
        "    \"\"\"Advanced AI Agent for scientific literature analysis using PaperQA2\"\"\"\n",
        "\n",
        "    def __init__(self, papers_directory: str, temperature: float = 0.1):\n",
        "        self.settings = create_gemini_settings(papers_directory, temperature)\n",
        "        self.papers_dir = papers_directory\n",
        "        print(f\"🤖 PaperQA Agent initialized with papers from: {papers_directory}\")\n",
        "\n",
        "    async def ask_question(self, question: str, use_agent: bool = True):\n",
        "        \"\"\"Ask a question about the research papers\"\"\"\n",
        "        print(f\"\\n❓ Question: {question}\")\n",
        "        print(\"🔍 Searching through research papers...\")\n",
        "\n",
        "        try:\n",
        "            if use_agent:\n",
        "                response = await agent_query(query=question, settings=self.settings)\n",
        "            else:\n",
        "                response = ask(question, settings=self.settings)\n",
        "\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing question: {e}\")\n",
        "            return None\n",
        "\n",
        "    def display_answer(self, response):\n",
        "        \"\"\"Display the answer with formatting\"\"\"\n",
        "        if response is None:\n",
        "            print(\"❌ No response received\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"📋 ANSWER:\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        answer_text = getattr(response, 'answer', str(response))\n",
        "        print(f\"\\n{answer_text}\")\n",
        "\n",
        "        contexts = getattr(response, 'contexts', getattr(response, 'context', []))\n",
        "        if contexts:\n",
        "            print(\"\\n\" + \"-\"*40)\n",
        "            print(\"📚 SOURCES USED:\")\n",
        "            print(\"-\"*40)\n",
        "            for i, context in enumerate(contexts[:3], 1):\n",
        "                context_name = getattr(context, 'name', getattr(context, 'doc', f'Source {i}'))\n",
        "                context_text = getattr(context, 'text', getattr(context, 'content', str(context)))\n",
        "                print(f\"\\n{i}. {context_name}\")\n",
        "                print(f\"   Text preview: {context_text[:150]}...\")\n",
        "\n",
        "    async def multi_question_analysis(self, questions: list):\n",
        "        \"\"\"Analyze multiple questions in sequence\"\"\"\n",
        "        results = {}\n",
        "        for i, question in enumerate(questions, 1):\n",
        "            print(f\"\\n🔄 Processing question {i}/{len(questions)}\")\n",
        "            response = await self.ask_question(question)\n",
        "            results[question] = response\n",
        "\n",
        "            if response:\n",
        "                print(f\"✅ Completed: {question[:50]}...\")\n",
        "            else:\n",
        "                print(f\"❌ Failed: {question[:50]}...\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    async def comparative_analysis(self, topic: str):\n",
        "        \"\"\"Perform comparative analysis across papers\"\"\"\n",
        "        questions = [\n",
        "            f\"What are the key innovations in {topic}?\",\n",
        "            f\"What are the limitations of current {topic} approaches?\",\n",
        "            f\"What future research directions are suggested for {topic}?\",\n",
        "        ]\n",
        "\n",
        "        print(f\"\\n🔬 Starting comparative analysis on: {topic}\")\n",
        "        return await self.multi_question_analysis(questions)\n",
        "\n",
        "async def basic_demo():\n",
        "    \"\"\"Demonstrate basic PaperQA functionality\"\"\"\n",
        "    agent = PaperQAAgent(papers_directory)\n",
        "\n",
        "    question = \"What is the transformer architecture and why is it important?\"\n",
        "    response = await agent.ask_question(question)\n",
        "    agent.display_answer(response)\n",
        "\n",
        "print(\"🚀 Running basic demonstration...\")\n",
        "await basic_demo()\n",
        "\n",
        "async def advanced_demo():\n",
        "    \"\"\"Demonstrate advanced multi-question analysis\"\"\"\n",
        "    agent = PaperQAAgent(papers_directory, temperature=0.2)\n",
        "\n",
        "    questions = [\n",
        "        \"How do attention mechanisms work in transformers?\",\n",
        "        \"What are the key differences between BERT and GPT models?\",\n",
        "        \"What are the computational challenges of large language models?\",\n",
        "        \"How has pre-training evolved in natural language processing?\"\n",
        "    ]\n",
        "\n",
        "    print(\"🧠 Running advanced multi-question analysis...\")\n",
        "    results = await agent.multi_question_analysis(questions)\n",
        "\n",
        "    for question, response in results.items():\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Q: {question}\")\n",
        "        print('='*80)\n",
        "        if response:\n",
        "            answer_text = getattr(response, 'answer', str(response))\n",
        "            display_text = answer_text[:300] + \"...\" if len(answer_text) > 300 else answer_text\n",
        "            print(display_text)\n",
        "        else:\n",
        "            print(\"❌ No answer available\")\n",
        "\n",
        "print(\"\\n🚀 Running advanced demonstration...\")\n",
        "await advanced_demo()\n",
        "\n",
        "async def research_comparison_demo():\n",
        "    \"\"\"Demonstrate comparative research analysis\"\"\"\n",
        "    agent = PaperQAAgent(papers_directory)\n",
        "\n",
        "    results = await agent.comparative_analysis(\"attention mechanisms in neural networks\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"📊 COMPARATIVE ANALYSIS RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for question, response in results.items():\n",
        "        print(f\"\\n🔍 {question}\")\n",
        "        print(\"-\" * 50)\n",
        "        if response:\n",
        "            answer_text = getattr(response, 'answer', str(response))\n",
        "            print(answer_text)\n",
        "        else:\n",
        "            print(\"❌ Analysis unavailable\")\n",
        "        print()\n",
        "\n",
        "print(\"🚀 Running comparative research analysis...\")\n",
        "await research_comparison_demo()"
      ],
      "metadata": {
        "id": "ztm21LZoQjti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8eZbiScMH-L",
        "outputId": "1e5da376-3ee9-46dd-9673-c7c0af563332"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92m15:12:47 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:12:47 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:12:47 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:12:47 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Gemini API key configured successfully!\n",
            "📥 Downloading sample research papers...\n",
            "📄 Already exists: attention_is_all_you_need.pdf\n",
            "📄 Already exists: bert_paper.pdf\n",
            "📄 Already exists: gpt3_paper.pdf\n",
            "🚀 Running basic demonstration...\n",
            "🤖 PaperQA Agent initialized with papers from: sample_papers\n",
            "\n",
            "❓ Question: What is the transformer architecture and why is it important?\n",
            "🔍 Searching through research papers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "ImportError: cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "ERROR:LiteLLM:Error creating standard logging object - cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "ImportError: cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "\u001b[92m15:12:50 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "ImportError: cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "ERROR:LiteLLM:Error creating standard logging object - cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "ImportError: cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "\u001b[92m15:12:50 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:50 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:12:50 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:12:50 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:12:50 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:12:50 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:50 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "ImportError: cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "ERROR:LiteLLM:Error creating standard logging object - cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "ImportError: cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "\u001b[92m15:12:50 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "ImportError: cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "ERROR:LiteLLM:Error creating standard logging object - cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "ImportError: cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:50 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:54 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:54 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:54 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:54 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:54 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:54 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:56 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:56 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:56 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:56 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:56 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:57 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:57 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:58 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:58 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:58 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:58 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:58 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:59 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:59 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:12:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:00 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:00 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:00 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:00 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:00 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:01 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:01 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:01 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:13:01 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:13:01 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:13:01 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:13:03 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:03 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:05 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:05 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:05 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:13:05 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:13:05 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:13:05 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "📋 ANSWER:\n",
            "============================================================\n",
            "\n",
            "session=PQASession(id=UUID('bab3d088-a73b-4758-9f0d-d555792a21c2'), question='What is the transformer architecture and why is it important?', answer=\"The Transformer architecture is a novel neural network design for sequence transduction that relies solely on an attention mechanism, eliminating recurrence and convolutions (vaswani2017attentionisall pages 9-10, vaswani2017attentionisall pages 2-3, vaswani2017attentionisall pages 1-2).  This reliance on attention allows for significantly greater parallelization during training, leading to faster training times and improved efficiency (vaswani2017attentionisall pages 9-10, vaswani2017attentionisall pages 2-3, vaswani2017attentionisall pages 1-2).  The Transformer's architecture, particularly its multi-headed self-attention mechanism, enables the processing of input and output representations without recurrent neural networks (RNNs) or convolutional layers (vaswani2017attentionisall pages 9-10, vaswani2017attentionisall pages 2-3).  This innovation resulted in state-of-the-art results in machine translation tasks (vaswani2017attentionisall pages 9-10, vaswani2017attentionisall pages 2-3, vaswani2017attentionisall pages 1-2).  Furthermore, the Transformer's architecture serves as the foundation for models like BERT, a multi-layer bidirectional Transformer encoder used for effective transfer learning in various natural language processing tasks (devlin2019bertpretrainingof pages 2-3).\\n\", raw_answer=\"The Transformer architecture is a novel neural network design for sequence transduction that relies solely on an attention mechanism, eliminating recurrence and convolutions (pqac-4bd5d9d9, pqac-eda20b10, pqac-8bb9fd08).  This reliance on attention allows for significantly greater parallelization during training, leading to faster training times and improved efficiency (pqac-4bd5d9d9, pqac-eda20b10, pqac-8bb9fd08).  The Transformer's architecture, particularly its multi-headed self-attention mechanism, enables the processing of input and output representations without recurrent neural networks (RNNs) or convolutional layers (pqac-eda20b10, pqac-8bb9fd08).  This innovation resulted in state-of-the-art results in machine translation tasks (pqac-4bd5d9d9, pqac-eda20b10, pqac-8bb9fd08).  Furthermore, the Transformer's architecture serves as the foundation for models like BERT, a multi-layer bidirectional Transformer encoder used for effective transfer learning in various natural language processing tasks (pqac-a7c92194).\\n\", answer_reasoning=None, has_successful_answer=True, context=\"pqac-4bd5d9d9: The Transformer is a novel network architecture based solely on attention mechanisms, eliminating recurrence and convolutions.  It surpasses existing models in quality and parallelization, significantly reducing training time.  In machine translation tasks, it achieved state-of-the-art results, showing superior performance and efficiency compared to recurrent or convolutional neural networks.\\nFrom Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and I. Polosukhin. Attention is all you need. ArXiv, pages 5998-6008, 2017. URL: https://doi.org/10.48550/arxiv.1706.03762, doi:10.48550/arxiv.1706.03762.\\n\\npqac-eda20b10: The Transformer is a novel neural network architecture for sequence transduction that eschews recurrence and convolution, relying entirely on an attention mechanism.  This allows for significantly more parallelization during training, achieving state-of-the-art translation quality in less time.  It's the first transduction model to use only self-attention to compute input and output representations without RNNs or convolutions.\\nFrom Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and I. Polosukhin. Attention is all you need. ArXiv, pages 5998-6008, 2017. URL: https://doi.org/10.48550/arxiv.1706.03762, doi:10.48550/arxiv.1706.03762.\\n\\npqac-8bb9fd08: The Transformer is the first sequence transduction model based entirely on attention, replacing recurrent layers in encoder-decoder architectures with multi-headed self-attention.  This allows for significantly faster training than architectures using recurrent or convolutional layers, achieving state-of-the-art results on translation tasks and showing promise for other applications.\\nFrom Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and I. Polosukhin. Attention is all you need. ArXiv, pages 5998-6008, 2017. URL: https://doi.org/10.48550/arxiv.1706.03762, doi:10.48550/arxiv.1706.03762.\\n\\npqac-a7c92194: BERT's architecture is a multi-layer bidirectional Transformer encoder, based on .  It uses a unified architecture across different tasks, minimizing differences between pre-training and downstream architectures.  This allows for effective transfer learning, where a model pre-trained on a large dataset is fine-tuned for a specific task, improving performance.\\nFrom Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. ArXiv, pages 4171-4186, 2019. URL: https://doi.org/10.48550/arxiv.1810.04805, doi:10.48550/arxiv.1810.04805.\\n\\nValid Keys: pqac-4bd5d9d9, pqac-eda20b10, pqac-8bb9fd08, pqac-a7c92194\", contexts=[Context(id='pqac-eda20b10', context=\"The Transformer is a novel neural network architecture for sequence transduction that eschews recurrence and convolution, relying entirely on an attention mechanism.  This allows for significantly more parallelization during training, achieving state-of-the-art translation quality in less time.  It's the first transduction model to use only self-attention to compute input and output representations without RNNs or convolutions.\", question='What is the transformer architecture and why is it important?', text=Text(text='', name='vaswani2017attentionisall pages 2-3', doc=DocDetails(docname='vaswani2017attentionisall', dockey='5e53125f15c47535', citation='Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and I. Polosukhin. Attention is all you need. ArXiv, pages 5998-6008, 2017. URL: https://doi.org/10.48550/arxiv.1706.03762, doi:10.48550/arxiv.1706.03762.', fields_to_overwrite_from_metadata={'key', 'docname', 'citation', 'dockey', 'doc_id'}, key='vaswani2017attentionisall', bibtex='@article{vaswani2017attentionisall,\\n    author = \"Vaswani, Ashish and Shazeer, Noam M. and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, I.\",\\n    title = \"Attention is All you Need\",\\n    year = \"2017\",\\n    journal = \"ArXiv\",\\n    pages = \"5998-6008\",\\n    doi = \"10.48550/arxiv.1706.03762\",\\n    url = \"https://doi.org/10.48550/arxiv.1706.03762\"\\n}\\n', authors=['Ashish Vaswani', 'Noam M. Shazeer', 'Niki Parmar', 'Jakob Uszkoreit', 'Llion Jones', 'Aidan N. Gomez', 'Lukasz Kaiser', 'I. Polosukhin'], publication_date=None, year=2017, volume=None, issue=None, issn=None, pages='5998-6008', journal='ArXiv', publisher=None, url=None, title='Attention is All you Need', citation_count=None, bibtex_type='article', source_quality=-1, is_retracted=None, doi='10.48550/arxiv.1706.03762', doi_url='https://doi.org/10.48550/arxiv.1706.03762', doc_id='5e53125f15c47535', file_location=None, license=None, pdf_url=None, other={'bibtex_source': ['self_generated', 'self_generated', 'self_generated'], 'paperId': '204e3073870fae3d05bcbc2f6a8e263d9b72e776', 'externalIds': {'MAG': '2963403868', 'DBLP': 'conf/nips/VaswaniSPUJGKP17', 'ArXiv': '1706.03762', 'CorpusId': 13756489}, 'matchScore': 132.94174, 'client_source': ['semantic_scholar']}, formatted_citation='Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and I. Polosukhin. Attention is all you need. ArXiv, pages 5998-6008, 2017. URL: https://doi.org/10.48550/arxiv.1706.03762, doi:10.48550/arxiv.1706.03762.')), score=10), Context(id='pqac-4bd5d9d9', context='The Transformer is a novel network architecture based solely on attention mechanisms, eliminating recurrence and convolutions.  It surpasses existing models in quality and parallelization, significantly reducing training time.  In machine translation tasks, it achieved state-of-the-art results, showing superior performance and efficiency compared to recurrent or convolutional neural networks.', question='What is the transformer architecture and why is it important?', text=Text(text='', name='vaswani2017attentionisall pages 1-2', doc=DocDetails(docname='vaswani2017attentionisall', dockey='5e53125f15c47535', citation='Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and I. Polosukhin. Attention is all you need. ArXiv, pages 5998-6008, 2017. URL: https://doi.org/10.48550/arxiv.1706.03762, doi:10.48550/arxiv.1706.03762.', fields_to_overwrite_from_metadata={'key', 'docname', 'citation', 'dockey', 'doc_id'}, key='vaswani2017attentionisall', bibtex='@article{vaswani2017attentionisall,\\n    author = \"Vaswani, Ashish and Shazeer, Noam M. and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, I.\",\\n    title = \"Attention is All you Need\",\\n    year = \"2017\",\\n    journal = \"ArXiv\",\\n    pages = \"5998-6008\",\\n    doi = \"10.48550/arxiv.1706.03762\",\\n    url = \"https://doi.org/10.48550/arxiv.1706.03762\"\\n}\\n', authors=['Ashish Vaswani', 'Noam M. Shazeer', 'Niki Parmar', 'Jakob Uszkoreit', 'Llion Jones', 'Aidan N. Gomez', 'Lukasz Kaiser', 'I. Polosukhin'], publication_date=None, year=2017, volume=None, issue=None, issn=None, pages='5998-6008', journal='ArXiv', publisher=None, url=None, title='Attention is All you Need', citation_count=None, bibtex_type='article', source_quality=-1, is_retracted=None, doi='10.48550/arxiv.1706.03762', doi_url='https://doi.org/10.48550/arxiv.1706.03762', doc_id='5e53125f15c47535', file_location=None, license=None, pdf_url=None, other={'bibtex_source': ['self_generated', 'self_generated', 'self_generated'], 'paperId': '204e3073870fae3d05bcbc2f6a8e263d9b72e776', 'externalIds': {'MAG': '2963403868', 'DBLP': 'conf/nips/VaswaniSPUJGKP17', 'ArXiv': '1706.03762', 'CorpusId': 13756489}, 'matchScore': 132.94174, 'client_source': ['semantic_scholar']}, formatted_citation='Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and I. Polosukhin. Attention is all you need. ArXiv, pages 5998-6008, 2017. URL: https://doi.org/10.48550/arxiv.1706.03762, doi:10.48550/arxiv.1706.03762.')), score=10), Context(id='pqac-8bb9fd08', context='The Transformer is the first sequence transduction model based entirely on attention, replacing recurrent layers in encoder-decoder architectures with multi-headed self-attention.  This allows for significantly faster training than architectures using recurrent or convolutional layers, achieving state-of-the-art results on translation tasks and showing promise for other applications.', question='What is the transformer architecture and why is it important?', text=Text(text='', name='vaswani2017attentionisall pages 9-10', doc=DocDetails(docname='vaswani2017attentionisall', dockey='5e53125f15c47535', citation='Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and I. Polosukhin. Attention is all you need. ArXiv, pages 5998-6008, 2017. URL: https://doi.org/10.48550/arxiv.1706.03762, doi:10.48550/arxiv.1706.03762.', fields_to_overwrite_from_metadata={'key', 'docname', 'citation', 'dockey', 'doc_id'}, key='vaswani2017attentionisall', bibtex='@article{vaswani2017attentionisall,\\n    author = \"Vaswani, Ashish and Shazeer, Noam M. and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, I.\",\\n    title = \"Attention is All you Need\",\\n    year = \"2017\",\\n    journal = \"ArXiv\",\\n    pages = \"5998-6008\",\\n    doi = \"10.48550/arxiv.1706.03762\",\\n    url = \"https://doi.org/10.48550/arxiv.1706.03762\"\\n}\\n', authors=['Ashish Vaswani', 'Noam M. Shazeer', 'Niki Parmar', 'Jakob Uszkoreit', 'Llion Jones', 'Aidan N. Gomez', 'Lukasz Kaiser', 'I. Polosukhin'], publication_date=None, year=2017, volume=None, issue=None, issn=None, pages='5998-6008', journal='ArXiv', publisher=None, url=None, title='Attention is All you Need', citation_count=None, bibtex_type='article', source_quality=-1, is_retracted=None, doi='10.48550/arxiv.1706.03762', doi_url='https://doi.org/10.48550/arxiv.1706.03762', doc_id='5e53125f15c47535', file_location=None, license=None, pdf_url=None, other={'bibtex_source': ['self_generated', 'self_generated', 'self_generated'], 'paperId': '204e3073870fae3d05bcbc2f6a8e263d9b72e776', 'externalIds': {'MAG': '2963403868', 'DBLP': 'conf/nips/VaswaniSPUJGKP17', 'ArXiv': '1706.03762', 'CorpusId': 13756489}, 'matchScore': 132.94174, 'client_source': ['semantic_scholar']}, formatted_citation='Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and I. Polosukhin. Attention is all you need. ArXiv, pages 5998-6008, 2017. URL: https://doi.org/10.48550/arxiv.1706.03762, doi:10.48550/arxiv.1706.03762.')), score=10), Context(id='pqac-9d6a1d4b', context='The Transformer architecture uses self-attention mechanisms, replacing recurrence and convolutions.  Experiments show variations in the number of attention heads, key/value dimensions, and model size impact performance. Larger models and dropout regularization improve results, while the choice of positional encoding has minimal effect.  The Transformer achieved state-of-the-art results on machine translation tasks, surpassing previous models in BLEU score and training efficiency.', question='What is the transformer architecture and why is it important?', text=Text(text='', name='vaswani2017attentionisall pages 8-9', doc=DocDetails(docname='vaswani2017attentionisall', dockey='5e53125f15c47535', citation='Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and I. Polosukhin. Attention is all you need. ArXiv, pages 5998-6008, 2017. URL: https://doi.org/10.48550/arxiv.1706.03762, doi:10.48550/arxiv.1706.03762.', fields_to_overwrite_from_metadata={'key', 'docname', 'citation', 'dockey', 'doc_id'}, key='vaswani2017attentionisall', bibtex='@article{vaswani2017attentionisall,\\n    author = \"Vaswani, Ashish and Shazeer, Noam M. and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, I.\",\\n    title = \"Attention is All you Need\",\\n    year = \"2017\",\\n    journal = \"ArXiv\",\\n    pages = \"5998-6008\",\\n    doi = \"10.48550/arxiv.1706.03762\",\\n    url = \"https://doi.org/10.48550/arxiv.1706.03762\"\\n}\\n', authors=['Ashish Vaswani', 'Noam M. Shazeer', 'Niki Parmar', 'Jakob Uszkoreit', 'Llion Jones', 'Aidan N. Gomez', 'Lukasz Kaiser', 'I. Polosukhin'], publication_date=None, year=2017, volume=None, issue=None, issn=None, pages='5998-6008', journal='ArXiv', publisher=None, url=None, title='Attention is All you Need', citation_count=None, bibtex_type='article', source_quality=-1, is_retracted=None, doi='10.48550/arxiv.1706.03762', doi_url='https://doi.org/10.48550/arxiv.1706.03762', doc_id='5e53125f15c47535', file_location=None, license=None, pdf_url=None, other={'bibtex_source': ['self_generated', 'self_generated', 'self_generated'], 'paperId': '204e3073870fae3d05bcbc2f6a8e263d9b72e776', 'externalIds': {'MAG': '2963403868', 'DBLP': 'conf/nips/VaswaniSPUJGKP17', 'ArXiv': '1706.03762', 'CorpusId': 13756489}, 'matchScore': 132.94174, 'client_source': ['semantic_scholar']}, formatted_citation='Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and I. Polosukhin. Attention is all you need. ArXiv, pages 5998-6008, 2017. URL: https://doi.org/10.48550/arxiv.1706.03762, doi:10.48550/arxiv.1706.03762.')), score=9), Context(id='pqac-dd419401', context='The Transformer architecture uses multi-head attention mechanisms in three ways: encoder-decoder attention, encoder self-attention, and decoder self-attention.  It replaces recurrence and convolutions, using positional encoding to incorporate sequence order.  Multi-head attention allows the model to attend to information from different representation subspaces, improving performance over single-head attention.', question='What is the transformer architecture and why is it important?', text=Text(text='', name='vaswani2017attentionisall pages 5-6', doc=DocDetails(docname='vaswani2017attentionisall', dockey='5e53125f15c47535', citation='Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and I. Polosukhin. Attention is all you need. ArXiv, pages 5998-6008, 2017. URL: https://doi.org/10.48550/arxiv.1706.03762, doi:10.48550/arxiv.1706.03762.', fields_to_overwrite_from_metadata={'key', 'docname', 'citation', 'dockey', 'doc_id'}, key='vaswani2017attentionisall', bibtex='@article{vaswani2017attentionisall,\\n    author = \"Vaswani, Ashish and Shazeer, Noam M. and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, I.\",\\n    title = \"Attention is All you Need\",\\n    year = \"2017\",\\n    journal = \"ArXiv\",\\n    pages = \"5998-6008\",\\n    doi = \"10.48550/arxiv.1706.03762\",\\n    url = \"https://doi.org/10.48550/arxiv.1706.03762\"\\n}\\n', authors=['Ashish Vaswani', 'Noam M. Shazeer', 'Niki Parmar', 'Jakob Uszkoreit', 'Llion Jones', 'Aidan N. Gomez', 'Lukasz Kaiser', 'I. Polosukhin'], publication_date=None, year=2017, volume=None, issue=None, issn=None, pages='5998-6008', journal='ArXiv', publisher=None, url=None, title='Attention is All you Need', citation_count=None, bibtex_type='article', source_quality=-1, is_retracted=None, doi='10.48550/arxiv.1706.03762', doi_url='https://doi.org/10.48550/arxiv.1706.03762', doc_id='5e53125f15c47535', file_location=None, license=None, pdf_url=None, other={'bibtex_source': ['self_generated', 'self_generated', 'self_generated'], 'paperId': '204e3073870fae3d05bcbc2f6a8e263d9b72e776', 'externalIds': {'MAG': '2963403868', 'DBLP': 'conf/nips/VaswaniSPUJGKP17', 'ArXiv': '1706.03762', 'CorpusId': 13756489}, 'matchScore': 132.94174, 'client_source': ['semantic_scholar']}, formatted_citation='Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and I. Polosukhin. Attention is all you need. ArXiv, pages 5998-6008, 2017. URL: https://doi.org/10.48550/arxiv.1706.03762, doi:10.48550/arxiv.1706.03762.')), score=9), Context(id='pqac-c29cc0fa', context='The Transformer architecture uses self-attention mechanisms, replacing recurrence and convolutions.  Self-attention allows for parallel computation, unlike recurrent layers, and is computationally comparable to separable convolutions.  The Transformer achieved state-of-the-art BLEU scores on WMT 2014 English-to-German and English-to-French translation tasks, significantly outperforming previous models.', question='What is the transformer architecture and why is it important?', text=Text(text='', name='vaswani2017attentionisall pages 7-8', doc=DocDetails(docname='vaswani2017attentionisall', dockey='5e53125f15c47535', citation='Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and I. Polosukhin. Attention is all you need. ArXiv, pages 5998-6008, 2017. URL: https://doi.org/10.48550/arxiv.1706.03762, doi:10.48550/arxiv.1706.03762.', fields_to_overwrite_from_metadata={'key', 'docname', 'citation', 'dockey', 'doc_id'}, key='vaswani2017attentionisall', bibtex='@article{vaswani2017attentionisall,\\n    author = \"Vaswani, Ashish and Shazeer, Noam M. and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, I.\",\\n    title = \"Attention is All you Need\",\\n    year = \"2017\",\\n    journal = \"ArXiv\",\\n    pages = \"5998-6008\",\\n    doi = \"10.48550/arxiv.1706.03762\",\\n    url = \"https://doi.org/10.48550/arxiv.1706.03762\"\\n}\\n', authors=['Ashish Vaswani', 'Noam M. Shazeer', 'Niki Parmar', 'Jakob Uszkoreit', 'Llion Jones', 'Aidan N. Gomez', 'Lukasz Kaiser', 'I. Polosukhin'], publication_date=None, year=2017, volume=None, issue=None, issn=None, pages='5998-6008', journal='ArXiv', publisher=None, url=None, title='Attention is All you Need', citation_count=None, bibtex_type='article', source_quality=-1, is_retracted=None, doi='10.48550/arxiv.1706.03762', doi_url='https://doi.org/10.48550/arxiv.1706.03762', doc_id='5e53125f15c47535', file_location=None, license=None, pdf_url=None, other={'bibtex_source': ['self_generated', 'self_generated', 'self_generated'], 'paperId': '204e3073870fae3d05bcbc2f6a8e263d9b72e776', 'externalIds': {'MAG': '2963403868', 'DBLP': 'conf/nips/VaswaniSPUJGKP17', 'ArXiv': '1706.03762', 'CorpusId': 13756489}, 'matchScore': 132.94174, 'client_source': ['semantic_scholar']}, formatted_citation='Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and I. Polosukhin. Attention is all you need. ArXiv, pages 5998-6008, 2017. URL: https://doi.org/10.48550/arxiv.1706.03762, doi:10.48550/arxiv.1706.03762.')), score=9), Context(id='pqac-a7c92194', context=\"BERT's architecture is a multi-layer bidirectional Transformer encoder, based on .  It uses a unified architecture across different tasks, minimizing differences between pre-training and downstream architectures.  This allows for effective transfer learning, where a model pre-trained on a large dataset is fine-tuned for a specific task, improving performance.\", question='What is the transformer architecture and why is it important?', text=Text(text='', name='devlin2019bertpretrainingof pages 2-3', doc=DocDetails(docname='devlin2019bertpretrainingof', dockey='3bedd9f94451f164', citation='Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. ArXiv, pages 4171-4186, 2019. URL: https://doi.org/10.48550/arxiv.1810.04805, doi:10.48550/arxiv.1810.04805.', fields_to_overwrite_from_metadata={'key', 'docname', 'citation', 'dockey', 'doc_id'}, key='devlin2019bertpretrainingof', bibtex='@article{devlin2019bertpretrainingof,\\n    author = \"Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina\",\\n    title = \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\\n    year = \"2019\",\\n    journal = \"ArXiv\",\\n    pages = \"4171-4186\",\\n    doi = \"10.48550/arxiv.1810.04805\",\\n    url = \"https://doi.org/10.48550/arxiv.1810.04805\"\\n}\\n', authors=['Jacob Devlin', 'Ming-Wei Chang', 'Kenton Lee', 'Kristina Toutanova'], publication_date=None, year=2019, volume=None, issue=None, issn=None, pages='4171-4186', journal='ArXiv', publisher=None, url=None, title='BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', citation_count=None, bibtex_type='article', source_quality=-1, is_retracted=None, doi='10.48550/arxiv.1810.04805', doi_url='https://doi.org/10.48550/arxiv.1810.04805', doc_id='3bedd9f94451f164', file_location=None, license=None, pdf_url=None, other={'bibtex_source': ['self_generated', 'self_generated', 'self_generated'], 'paperId': 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', 'externalIds': {'MAG': '2951055169', 'ACL': 'N19-1423', 'DBLP': 'journals/corr/abs-1810-04805', 'ArXiv': '1810.04805', 'DOI': '10.18653/v1/N19-1423', 'CorpusId': 52967399}, 'matchScore': 239.70224, 'client_source': ['semantic_scholar']}, formatted_citation='Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. ArXiv, pages 4171-4186, 2019. URL: https://doi.org/10.48550/arxiv.1810.04805, doi:10.48550/arxiv.1810.04805.')), score=9), Context(id='pqac-b1a03844', context=\"BERT uses a bidirectional Transformer architecture, unlike GPT which uses constrained, left-to-right self-attention.  This bidirectional approach allows BERT to consider context from both sides of a word, enabling a deeper understanding of language.  BERT's architecture is important because it allows for more powerful deep bidirectional representations compared to left-to-right or shallow concatenated models.\", question='What is the transformer architecture and why is it important?', text=Text(text='', name='devlin2019bertpretrainingof pages 3-4', doc=DocDetails(docname='devlin2019bertpretrainingof', dockey='3bedd9f94451f164', citation='Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. ArXiv, pages 4171-4186, 2019. URL: https://doi.org/10.48550/arxiv.1810.04805, doi:10.48550/arxiv.1810.04805.', fields_to_overwrite_from_metadata={'key', 'docname', 'citation', 'dockey', 'doc_id'}, key='devlin2019bertpretrainingof', bibtex='@article{devlin2019bertpretrainingof,\\n    author = \"Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina\",\\n    title = \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\\n    year = \"2019\",\\n    journal = \"ArXiv\",\\n    pages = \"4171-4186\",\\n    doi = \"10.48550/arxiv.1810.04805\",\\n    url = \"https://doi.org/10.48550/arxiv.1810.04805\"\\n}\\n', authors=['Jacob Devlin', 'Ming-Wei Chang', 'Kenton Lee', 'Kristina Toutanova'], publication_date=None, year=2019, volume=None, issue=None, issn=None, pages='4171-4186', journal='ArXiv', publisher=None, url=None, title='BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', citation_count=None, bibtex_type='article', source_quality=-1, is_retracted=None, doi='10.48550/arxiv.1810.04805', doi_url='https://doi.org/10.48550/arxiv.1810.04805', doc_id='3bedd9f94451f164', file_location=None, license=None, pdf_url=None, other={'bibtex_source': ['self_generated', 'self_generated', 'self_generated'], 'paperId': 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', 'externalIds': {'MAG': '2951055169', 'ACL': 'N19-1423', 'DBLP': 'journals/corr/abs-1810-04805', 'ArXiv': '1810.04805', 'DOI': '10.18653/v1/N19-1423', 'CorpusId': 52967399}, 'matchScore': 239.70224, 'client_source': ['semantic_scholar']}, formatted_citation='Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. ArXiv, pages 4171-4186, 2019. URL: https://doi.org/10.48550/arxiv.1810.04805, doi:10.48550/arxiv.1810.04805.')), score=9)], references='1. (vaswani2017attentionisall pages 9-10): Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and I. Polosukhin. Attention is all you need. ArXiv, pages 5998-6008, 2017. URL: https://doi.org/10.48550/arxiv.1706.03762, doi:10.48550/arxiv.1706.03762.\\n\\n2. (vaswani2017attentionisall pages 2-3): Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and I. Polosukhin. Attention is all you need. ArXiv, pages 5998-6008, 2017. URL: https://doi.org/10.48550/arxiv.1706.03762, doi:10.48550/arxiv.1706.03762.\\n\\n3. (vaswani2017attentionisall pages 1-2): Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and I. Polosukhin. Attention is all you need. ArXiv, pages 5998-6008, 2017. URL: https://doi.org/10.48550/arxiv.1706.03762, doi:10.48550/arxiv.1706.03762.\\n\\n4. (devlin2019bertpretrainingof pages 2-3): Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. ArXiv, pages 4171-4186, 2019. URL: https://doi.org/10.48550/arxiv.1810.04805, doi:10.48550/arxiv.1810.04805.', formatted_answer=\"Question: What is the transformer architecture and why is it important?\\n\\nThe Transformer architecture is a novel neural network design for sequence transduction that relies solely on an attention mechanism, eliminating recurrence and convolutions (vaswani2017attentionisall pages 9-10, vaswani2017attentionisall pages 2-3, vaswani2017attentionisall pages 1-2).  This reliance on attention allows for significantly greater parallelization during training, leading to faster training times and improved efficiency (vaswani2017attentionisall pages 9-10, vaswani2017attentionisall pages 2-3, vaswani2017attentionisall pages 1-2).  The Transformer's architecture, particularly its multi-headed self-attention mechanism, enables the processing of input and output representations without recurrent neural networks (RNNs) or convolutional layers (vaswani2017attentionisall pages 9-10, vaswani2017attentionisall pages 2-3).  This innovation resulted in state-of-the-art results in machine translation tasks (vaswani2017attentionisall pages 9-10, vaswani2017attentionisall pages 2-3, vaswani2017attentionisall pages 1-2).  Furthermore, the Transformer's architecture serves as the foundation for models like BERT, a multi-layer bidirectional Transformer encoder used for effective transfer learning in various natural language processing tasks (devlin2019bertpretrainingof pages 2-3).\\n\\n\\nReferences\\n\\n1. (vaswani2017attentionisall pages 9-10): Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and I. Polosukhin. Attention is all you need. ArXiv, pages 5998-6008, 2017. URL: https://doi.org/10.48550/arxiv.1706.03762, doi:10.48550/arxiv.1706.03762.\\n\\n2. (vaswani2017attentionisall pages 2-3): Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and I. Polosukhin. Attention is all you need. ArXiv, pages 5998-6008, 2017. URL: https://doi.org/10.48550/arxiv.1706.03762, doi:10.48550/arxiv.1706.03762.\\n\\n3. (vaswani2017attentionisall pages 1-2): Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and I. Polosukhin. Attention is all you need. ArXiv, pages 5998-6008, 2017. URL: https://doi.org/10.48550/arxiv.1706.03762, doi:10.48550/arxiv.1706.03762.\\n\\n4. (devlin2019bertpretrainingof pages 2-3): Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. ArXiv, pages 4171-4186, 2019. URL: https://doi.org/10.48550/arxiv.1810.04805, doi:10.48550/arxiv.1810.04805.\\n\", graded_answer=None, cost=0.001250475, token_counts={'gemini/gemini-1.5-flash': [569, 39], 'gemini-1.5-flash': [11752, 1049]}, config_md5='41ea849f3f8f755869f306696d9e2d66', tool_history=[['paper_search', 'gather_evidence', 'gen_answer', 'complete']], used_contexts={'pqac-eda20b10', 'pqac-4bd5d9d9', 'pqac-8bb9fd08', 'pqac-a7c92194'}) bibtex=None status=<AgentStatus.SUCCESS: 'success'> timing_info=None duration=0.0 stats=None\n",
            "\n",
            "🚀 Running advanced demonstration...\n",
            "🤖 PaperQA Agent initialized with papers from: sample_papers\n",
            "🧠 Running advanced multi-question analysis...\n",
            "\n",
            "🔄 Processing question 1/4\n",
            "\n",
            "❓ Question: How do attention mechanisms work in transformers?\n",
            "🔍 Searching through research papers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"13s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"13s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:46 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"13s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"13s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"13s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"13s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:53 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:53 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:53 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:53 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:53 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:53 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:54 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:54 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:54 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:55 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:55 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:55 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:55 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:55 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:56 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:56 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:57 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:57 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:57 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:58 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:58 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:58 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:59 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:59 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:59 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:13:59 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:13:59 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:00 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:00 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:00 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:14:00 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:14:00 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:14:00 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:14:02 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:02 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:04 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:04 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:04 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:14:04 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:14:04 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:14:04 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Completed: How do attention mechanisms work in transformers?...\n",
            "\n",
            "🔄 Processing question 2/4\n",
            "\n",
            "❓ Question: What are the key differences between BERT and GPT models?\n",
            "🔍 Searching through research papers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:52 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "ImportError: cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "ERROR:LiteLLM:Error creating standard logging object - cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "ImportError: cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "\u001b[92m15:14:52 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "ImportError: cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "ERROR:LiteLLM:Error creating standard logging object - cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "ImportError: cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "\u001b[92m15:14:52 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:52 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "ImportError: cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "ERROR:LiteLLM:Error creating standard logging object - cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "ImportError: cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:52 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:53 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "ImportError: cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "\u001b[92m15:14:53 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "ImportError: cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "ERROR:LiteLLM:Error creating standard logging object - cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "ImportError: cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "ERROR:LiteLLM:Error creating standard logging object - cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "ImportError: cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:53 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:54 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:54 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:55 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:55 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:55 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:55 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:55 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:55 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:56 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:56 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:56 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:56 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:56 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:57 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:57 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:57 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:57 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:58 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:58 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:58 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:58 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:58 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:59 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:59 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:14:59 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:15:00 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:15:00 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:15:00 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:00 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:15:00 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:15:01 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:15:01 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:01 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:15:01 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:15:01 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:15:01 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:15:04 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:15:04 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "ImportError: cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "ERROR:LiteLLM:Error creating standard logging object - cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "ImportError: cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "\u001b[92m15:15:04 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "ImportError: cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "ERROR:LiteLLM:Error creating standard logging object - cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "ImportError: cannot import name 'general_settings' from 'litellm.proxy.proxy_server' (/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py)\n",
            "\u001b[92m15:15:04 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:15:04 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:04 - LiteLLM Router:ERROR\u001b[0m: router.py:4376 - litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "ERROR:LiteLLM Router:litellm.router.Router::deployment_callback_on_success(): Exception occured - standard_logging_object is None\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4316, in deployment_callback_on_success\n",
            "    raise ValueError(\"standard_logging_object is None\")\n",
            "ValueError: standard_logging_object is None\n",
            "\u001b[92m15:15:07 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:15:07 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:15:07 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:15:07 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Completed: What are the key differences between BERT and GPT ...\n",
            "\n",
            "🔄 Processing question 3/4\n",
            "\n",
            "❓ Question: What are the computational challenges of large language models?\n",
            "🔍 Searching through research papers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"13s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"13s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:15:46 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"13s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"13s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"13s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"13s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:15:47 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"12s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"12s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"12s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"12s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:15:47 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"12s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"12s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"12s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"12s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:15:51 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"8s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"8s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"8s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"8s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:15:51 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"8s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"8s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"8s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"8s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:15:52 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"7s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"7s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"7s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"7s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:15:52 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"7s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"7s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"7s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"7s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:15:52 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"7s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"7s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"7s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"7s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:15:52 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"7s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"7s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"7s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"7s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:aviary.env:Encountered exception during tool call for tool gen_answer: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"15\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"24s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            ". Received Model Group=gemini/gemini-1.5-flash\n",
            "Available Model Group Fallbacks=None LiteLLM Retried: 2 times, LiteLLM Max Retries: 3\n",
            "\u001b[92m15:15:57 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:15:57 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:15:57 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:15:57 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Completed: What are the computational challenges of large lan...\n",
            "\n",
            "🔄 Processing question 4/4\n",
            "\n",
            "❓ Question: How has pre-training evolved in natural language processing?\n",
            "🔍 Searching through research papers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "        \"retryDelay\": \"39s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"27s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"27s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:16:32 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"38s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"38s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"27s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"27s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"38s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"38s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"27s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"27s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:16:32 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"38s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"38s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"27s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"27s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"38s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"38s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"27s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"27s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:16:37 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"39s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"39s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"39s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"39s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:16:37 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"39s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"39s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"39s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"39s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:16:37 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"39s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"39s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"39s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"39s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:16:37 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"39s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"39s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"39s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"39s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:16:37 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"38s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"38s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"38s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"38s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:16:37 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"38s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"38s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"38s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"38s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"22s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:16:42 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"17s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"17s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"17s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"17s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:16:42 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"17s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"17s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"17s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"17s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:aviary.env:Encountered exception during tool call for tool gen_answer: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"39s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            ". Received Model Group=gemini/gemini-1.5-flash\n",
            "Available Model Group Fallbacks=None LiteLLM Retried: 2 times, LiteLLM Max Retries: 3\n",
            "\u001b[92m15:16:43 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:16:43 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:16:43 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:16:43 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Completed: How has pre-training evolved in natural language p...\n",
            "\n",
            "================================================================================\n",
            "Q: How do attention mechanisms work in transformers?\n",
            "================================================================================\n",
            "session=PQASession(id=UUID('9f0c52a6-f8af-4187-bef7-819c3be7ff61'), question='How do attention mechanisms work in transformers?', answer='Transformer attention mechanisms map queries and key-value pairs to a weighted sum of values (vaswani2017attentionisall pages 3-5).  The weight for each value is ...\n",
            "\n",
            "================================================================================\n",
            "Q: What are the key differences between BERT and GPT models?\n",
            "================================================================================\n",
            "session=PQASession(id=UUID('1e2d4b5d-0e70-4dae-8844-f74b2734f925'), question='What are the key differences between BERT and GPT models?', answer=\"BERT employs bidirectional training, processing text in both directions simultaneously, unlike GPT's unidirectional left-to-right approach (devlin2019bert...\n",
            "\n",
            "================================================================================\n",
            "Q: What are the computational challenges of large language models?\n",
            "================================================================================\n",
            "session=PQASession(id=UUID('a79dbabe-df26-4a10-a0b0-65fb61713c96'), question='What are the computational challenges of large language models?', answer='No answer generated.', raw_answer='', answer_reasoning=None, has_successful_answer=True, context='', contexts=[], references='', formatted_answer=''...\n",
            "\n",
            "================================================================================\n",
            "Q: How has pre-training evolved in natural language processing?\n",
            "================================================================================\n",
            "session=PQASession(id=UUID('cdadb8a6-e14f-43fe-9190-6ac81c9a554e'), question='How has pre-training evolved in natural language processing?', answer='', raw_answer='', answer_reasoning=None, has_successful_answer=None, context='', contexts=[], references='', formatted_answer='', graded_answer=None, c...\n",
            "🚀 Running comparative research analysis...\n",
            "🤖 PaperQA Agent initialized with papers from: sample_papers\n",
            "\n",
            "🔬 Starting comparative analysis on: attention mechanisms in neural networks\n",
            "\n",
            "🔄 Processing question 1/3\n",
            "\n",
            "❓ Question: What are the key innovations in attention mechanisms in neural networks?\n",
            "🔍 Searching through research papers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"16s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"16s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"59s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"59s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"16s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"16s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"59s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"59s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:17:00 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"16s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"16s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"59s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"59s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"16s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"16s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"59s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"59s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:paperqa.agents.main:Trajectory failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"16s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/paperqa/agents/main.py\", line 158, in _run_with_timeout_failure\n",
            "    status = await rollout()\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/paperqa/agents/main.py\", line 309, in rollout\n",
            "    for attempt in Retrying(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\", line 443, in __iter__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\", line 376, in iter\n",
            "    result = action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\", line 398, in <lambda>\n",
            "    self._add_action_func(lambda rs: rs.outcome.result())\n",
            "                                     ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/paperqa/agents/main.py\", line 316, in rollout\n",
            "    action = await agent(agent_state.messages, tools)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/aviary/tools/utils.py\", line 96, in __call__\n",
            "    model_response = await self._bound_acompletion(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1083, in acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1059, in acompletion\n",
            "    response = await self.async_function_with_fallbacks(**kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3902, in async_function_with_fallbacks\n",
            "    return await self.async_function_with_fallbacks_common_utils(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3860, in async_function_with_fallbacks_common_utils\n",
            "    raise original_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3894, in async_function_with_fallbacks\n",
            "    response = await self.async_function_with_retries(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4099, in async_function_with_retries\n",
            "    raise original_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"16s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            ". Received Model Group=gemini/gemini-1.5-flash\n",
            "Available Model Group Fallbacks=None LiteLLM Retried: 2 times, LiteLLM Max Retries: 3\n",
            "\u001b[92m15:17:05 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:17:05 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:17:05 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:17:05 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:17:05 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:17:05 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:17:11 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"48s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"48s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"48s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"48s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:17:11 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"48s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"48s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"48s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"48s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:17:16 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"43s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"43s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"43s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"43s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:17:16 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"43s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"43s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"43s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"43s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:17:22 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"37s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"37s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"37s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"37s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:17:22 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"37s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"37s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"37s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"37s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:aviary.env:Encountered exception during tool call for tool gen_answer: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"54s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            ". Received Model Group=gemini/gemini-1.5-flash\n",
            "Available Model Group Fallbacks=None LiteLLM Retried: 2 times, LiteLLM Max Retries: 3\n",
            "\u001b[92m15:17:27 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:17:27 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:17:27 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:17:27 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Completed: What are the key innovations in attention mechanis...\n",
            "\n",
            "🔄 Processing question 2/3\n",
            "\n",
            "❓ Question: What are the limitations of current attention mechanisms in neural networks approaches?\n",
            "🔍 Searching through research papers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"31s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"31s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"14s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"14s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"31s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"31s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"14s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"14s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:17:45 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"31s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"31s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"14s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"14s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"31s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"31s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"14s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"14s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:paperqa.agents.main:Trajectory failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"31s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/paperqa/agents/main.py\", line 158, in _run_with_timeout_failure\n",
            "    status = await rollout()\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/paperqa/agents/main.py\", line 309, in rollout\n",
            "    for attempt in Retrying(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\", line 443, in __iter__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\", line 376, in iter\n",
            "    result = action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\", line 398, in <lambda>\n",
            "    self._add_action_func(lambda rs: rs.outcome.result())\n",
            "                                     ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/paperqa/agents/main.py\", line 316, in rollout\n",
            "    action = await agent(agent_state.messages, tools)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/aviary/tools/utils.py\", line 96, in __call__\n",
            "    model_response = await self._bound_acompletion(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1083, in acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1059, in acompletion\n",
            "    response = await self.async_function_with_fallbacks(**kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3902, in async_function_with_fallbacks\n",
            "    return await self.async_function_with_fallbacks_common_utils(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3860, in async_function_with_fallbacks_common_utils\n",
            "    raise original_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3894, in async_function_with_fallbacks\n",
            "    response = await self.async_function_with_retries(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4099, in async_function_with_retries\n",
            "    raise original_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"31s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            ". Received Model Group=gemini/gemini-1.5-flash\n",
            "Available Model Group Fallbacks=None LiteLLM Retried: 2 times, LiteLLM Max Retries: 3\n",
            "\u001b[92m15:17:50 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:17:50 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:17:50 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:17:50 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:17:50 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:17:51 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:17:56 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"3s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"3s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"3s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"3s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:17:56 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"3s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"3s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"3s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"3s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:18:02 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"58s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"58s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"58s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"58s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:18:02 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"58s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"58s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"58s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"58s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:18:07 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"52s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"52s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"52s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"52s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:18:07 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"52s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"52s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"52s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"52s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:aviary.env:Encountered exception during tool call for tool gen_answer: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            ". Received Model Group=gemini/gemini-1.5-flash\n",
            "Available Model Group Fallbacks=None LiteLLM Retried: 2 times, LiteLLM Max Retries: 3\n",
            "\u001b[92m15:18:12 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:18:12 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:18:12 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:18:12 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Completed: What are the limitations of current attention mech...\n",
            "\n",
            "🔄 Processing question 3/3\n",
            "\n",
            "❓ Question: What future research directions are suggested for attention mechanisms in neural networks?\n",
            "🔍 Searching through research papers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"46s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"46s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"30s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"30s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"46s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"46s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"30s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"30s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:18:29 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"46s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"46s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"30s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"30s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"46s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"46s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"30s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"30s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:paperqa.agents.main:Trajectory failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"46s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/paperqa/agents/main.py\", line 158, in _run_with_timeout_failure\n",
            "    status = await rollout()\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/paperqa/agents/main.py\", line 309, in rollout\n",
            "    for attempt in Retrying(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\", line 443, in __iter__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\", line 376, in iter\n",
            "    result = action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\", line 398, in <lambda>\n",
            "    self._add_action_func(lambda rs: rs.outcome.result())\n",
            "                                     ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/paperqa/agents/main.py\", line 316, in rollout\n",
            "    action = await agent(agent_state.messages, tools)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/aviary/tools/utils.py\", line 96, in __call__\n",
            "    model_response = await self._bound_acompletion(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1083, in acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1059, in acompletion\n",
            "    response = await self.async_function_with_fallbacks(**kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3902, in async_function_with_fallbacks\n",
            "    return await self.async_function_with_fallbacks_common_utils(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3860, in async_function_with_fallbacks_common_utils\n",
            "    raise original_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3894, in async_function_with_fallbacks\n",
            "    response = await self.async_function_with_retries(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4099, in async_function_with_retries\n",
            "    raise original_exception\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"46s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            ". Received Model Group=gemini/gemini-1.5-flash\n",
            "Available Model Group Fallbacks=None LiteLLM Retried: 2 times, LiteLLM Max Retries: 3\n",
            "\u001b[92m15:18:34 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:18:34 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:18:34 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:18:34 - LiteLLM:WARNING\u001b[0m: logging_callback_manager.py:135 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "WARNING:LiteLLM:Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30\n",
            "\u001b[92m15:18:34 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:18:34 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:18:39 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"20s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"20s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"20s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"20s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:18:39 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"20s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"20s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"20s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"20s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:18:45 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"14s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"14s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"14s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"14s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:18:45 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"14s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"14s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"14s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"14s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:18:50 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "\u001b[92m15:18:50 - LiteLLM:ERROR\u001b[0m: litellm_logging.py:4483 - Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:LiteLLM:Error creating standard logging object - Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 3990, in async_function_with_retries\n",
            "    response = await self.make_call(original_function, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 4108, in make_call\n",
            "    response = await response\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1364, in _acompletion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/router.py\", line 1316, in _acompletion\n",
            "    response = await _response\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1586, in wrapper_async\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1686, in async_completion\n",
            "    response = await client.post(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/logging_utils.py\", line 135, in async_wrapper\n",
            "    result = await func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 324, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 280, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyB7iRl7wqRcZoNxGVnump0l3CrKoxbH8Hs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 544, in acompletion\n",
            "    response = await init_response\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1692, in async_completion\n",
            "    raise VertexAIError(\n",
            "litellm.llms.vertex_ai.common_utils.VertexAIError: {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 1437, in wrapper_async\n",
            "    result = await original_function(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 563, in acompletion\n",
            "    raise exception_type(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2301, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1330, in exception_type\n",
            "    raise RateLimitError(\n",
            "litellm.exceptions.RateLimitError: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"location\": \"global\",\n",
            "              \"model\": \"gemini-1.5-flash\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"9s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 74, in <module>\n",
            "    import backoff\n",
            "ModuleNotFoundError: No module named 'backoff'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4370, in get_standard_logging_object_payload\n",
            "    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 3921, in get_standard_logging_metadata\n",
            "    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/litellm_logging.py\", line 4109, in _generate_cold_storage_object_key\n",
            "    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/spend_tracking/cold_storage_handler.py\", line 67, in _get_configured_cold_storage_custom_logger\n",
            "    from litellm.proxy.proxy_server import general_settings\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/litellm/proxy/proxy_server.py\", line 80, in <module>\n",
            "    raise ImportError(f\"Missing dependency {e}. Run `pip install 'litellm[proxy]'`\")\n",
            "ImportError: Missing dependency No module named 'backoff'. Run `pip install 'litellm[proxy]'`\n",
            "ERROR:aviary.env:Encountered exception during tool call for tool gen_answer: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
            "  \"error\": {\n",
            "    \"code\": 429,\n",
            "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
            "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
            "    \"details\": [\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
            "        \"violations\": [\n",
            "          {\n",
            "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
            "            \"quotaId\": \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\",\n",
            "            \"quotaDimensions\": {\n",
            "              \"model\": \"gemini-1.5-flash\",\n",
            "              \"location\": \"global\"\n",
            "            },\n",
            "            \"quotaValue\": \"50\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
            "        \"links\": [\n",
            "          {\n",
            "            \"description\": \"Learn more about Gemini API quotas\",\n",
            "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
            "        \"retryDelay\": \"25s\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            ". Received Model Group=gemini/gemini-1.5-flash\n",
            "Available Model Group Fallbacks=None LiteLLM Retried: 2 times, LiteLLM Max Retries: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Completed: What future research directions are suggested for ...\n",
            "\n",
            "================================================================================\n",
            "📊 COMPARATIVE ANALYSIS RESULTS\n",
            "================================================================================\n",
            "\n",
            "🔍 What are the key innovations in attention mechanisms in neural networks?\n",
            "--------------------------------------------------\n",
            "session=PQASession(id=UUID('526309b8-3f06-4702-ad51-ebf4b8c009cf'), question='What are the key innovations in attention mechanisms in neural networks?', answer='', raw_answer='', answer_reasoning=None, has_successful_answer=None, context='', contexts=[], references='', formatted_answer='', graded_answer=None, cost=0.0, token_counts={}, config_md5='41ea849f3f8f755869f306696d9e2d66', tool_history=[['gen_answer']], used_contexts=set()) bibtex=None status=<AgentStatus.FAIL: 'fail'> timing_info=None duration=0.0 stats=None\n",
            "\n",
            "\n",
            "🔍 What are the limitations of current attention mechanisms in neural networks approaches?\n",
            "--------------------------------------------------\n",
            "session=PQASession(id=UUID('c0499799-9852-4def-a185-83b1b17b0b4e'), question='What are the limitations of current attention mechanisms in neural networks approaches?', answer='', raw_answer='', answer_reasoning=None, has_successful_answer=None, context='', contexts=[], references='', formatted_answer='', graded_answer=None, cost=0.0, token_counts={}, config_md5='41ea849f3f8f755869f306696d9e2d66', tool_history=[['gen_answer']], used_contexts=set()) bibtex=None status=<AgentStatus.FAIL: 'fail'> timing_info=None duration=0.0 stats=None\n",
            "\n",
            "\n",
            "🔍 What future research directions are suggested for attention mechanisms in neural networks?\n",
            "--------------------------------------------------\n",
            "session=PQASession(id=UUID('1b50fe65-c07c-4cea-b034-5094218487c7'), question='What future research directions are suggested for attention mechanisms in neural networks?', answer='', raw_answer='', answer_reasoning=None, has_successful_answer=None, context='', contexts=[], references='', formatted_answer='', graded_answer=None, cost=0.0, token_counts={}, config_md5='41ea849f3f8f755869f306696d9e2d66', tool_history=[['gen_answer']], used_contexts=set()) bibtex=None status=<AgentStatus.FAIL: 'fail'> timing_info=None duration=0.0 stats=None\n",
            "\n",
            "🤖 PaperQA Agent initialized with papers from: sample_papers\n",
            "\n",
            "🎯 Interactive agent ready! You can now ask custom questions:\n",
            "Example: await interactive_query('How do transformers handle long sequences?')\n",
            "\n",
            "    🎯 USAGE TIPS FOR PAPERQA2 WITH GEMINI:\n",
            "    \n",
            "    1. 📝 Question Formulation:\n",
            "       - Be specific about what you want to know\n",
            "       - Ask about comparisons, mechanisms, or implications\n",
            "       - Use domain-specific terminology\n",
            "    \n",
            "    2. 🔧 Model Configuration:\n",
            "       - Gemini 1.5 Flash is free and reliable\n",
            "       - Adjust temperature (0.0-1.0) for creativity vs precision\n",
            "       - Use smaller chunk_size for better processing\n",
            "    \n",
            "    3. 📚 Document Management:\n",
            "       - Add PDFs to the papers directory\n",
            "       - Use meaningful filenames\n",
            "       - Mix different types of papers for better coverage\n",
            "    \n",
            "    4. ⚡ Performance Optimization:\n",
            "       - Limit concurrent requests for free tier\n",
            "       - Use smaller evidence_k values for faster responses\n",
            "       - Cache results by saving the agent state\n",
            "    \n",
            "    5. 🧠 Advanced Usage:\n",
            "       - Chain multiple questions for deeper analysis\n",
            "       - Use comparative analysis for research reviews\n",
            "       - Combine with other tools for complete workflows\n",
            "    \n",
            "    📖 Example Questions to Try:\n",
            "    - \"Compare the attention mechanisms in BERT vs GPT models\"\n",
            "    - \"What are the computational bottlenecks in transformer training?\"\n",
            "    - \"How has pre-training evolved from word2vec to modern LLMs?\"\n",
            "    - \"What are the key innovations that made transformers successful?\"\n",
            "    \n",
            "✅ Tutorial complete! You now have a fully functional PaperQA2 AI Agent with Gemini.\n",
            "🔗 Remember to get your free Gemini API key from: https://aistudio.google.com/app/apikey\n"
          ]
        }
      ],
      "source": [
        "def create_interactive_agent():\n",
        "    \"\"\"Create an interactive agent for custom queries\"\"\"\n",
        "    agent = PaperQAAgent(papers_directory)\n",
        "\n",
        "    async def query(question: str, show_sources: bool = True):\n",
        "        \"\"\"Interactive query function\"\"\"\n",
        "        response = await agent.ask_question(question)\n",
        "\n",
        "        if response:\n",
        "            answer_text = getattr(response, 'answer', str(response))\n",
        "            print(f\"\\n🤖 Answer:\\n{answer_text}\")\n",
        "\n",
        "            if show_sources:\n",
        "                contexts = getattr(response, 'contexts', getattr(response, 'context', []))\n",
        "                if contexts:\n",
        "                    print(f\"\\n📚 Based on {len(contexts)} sources:\")\n",
        "                    for i, ctx in enumerate(contexts[:3], 1):\n",
        "                        ctx_name = getattr(ctx, 'name', getattr(ctx, 'doc', f'Source {i}'))\n",
        "                        print(f\"  {i}. {ctx_name}\")\n",
        "        else:\n",
        "            print(\"❌ Sorry, I couldn't find an answer to that question.\")\n",
        "\n",
        "        return response\n",
        "\n",
        "    return query\n",
        "\n",
        "interactive_query = create_interactive_agent()\n",
        "\n",
        "print(\"\\n🎯 Interactive agent ready! You can now ask custom questions:\")\n",
        "print(\"Example: await interactive_query('How do transformers handle long sequences?')\")\n",
        "\n",
        "def print_usage_tips():\n",
        "    \"\"\"Print helpful usage tips\"\"\"\n",
        "    tips = \"\"\"\n",
        "    🎯 USAGE TIPS FOR PAPERQA2 WITH GEMINI:\n",
        "\n",
        "    1. 📝 Question Formulation:\n",
        "       - Be specific about what you want to know\n",
        "       - Ask about comparisons, mechanisms, or implications\n",
        "       - Use domain-specific terminology\n",
        "\n",
        "    2. 🔧 Model Configuration:\n",
        "       - Gemini 1.5 Flash is free and reliable\n",
        "       - Adjust temperature (0.0-1.0) for creativity vs precision\n",
        "       - Use smaller chunk_size for better processing\n",
        "\n",
        "    3. 📚 Document Management:\n",
        "       - Add PDFs to the papers directory\n",
        "       - Use meaningful filenames\n",
        "       - Mix different types of papers for better coverage\n",
        "\n",
        "    4. ⚡ Performance Optimization:\n",
        "       - Limit concurrent requests for free tier\n",
        "       - Use smaller evidence_k values for faster responses\n",
        "       - Cache results by saving the agent state\n",
        "\n",
        "    5. 🧠 Advanced Usage:\n",
        "       - Chain multiple questions for deeper analysis\n",
        "       - Use comparative analysis for research reviews\n",
        "       - Combine with other tools for complete workflows\n",
        "\n",
        "    📖 Example Questions to Try:\n",
        "    - \"Compare the attention mechanisms in BERT vs GPT models\"\n",
        "    - \"What are the computational bottlenecks in transformer training?\"\n",
        "    - \"How has pre-training evolved from word2vec to modern LLMs?\"\n",
        "    - \"What are the key innovations that made transformers successful?\"\n",
        "    \"\"\"\n",
        "    print(tips)\n",
        "\n",
        "print_usage_tips()\n",
        "\n",
        "def save_analysis_results(results: dict, filename: str = \"paperqa_analysis.txt\"):\n",
        "    \"\"\"Save analysis results to a file\"\"\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"PaperQA2 Analysis Results\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "        for question, response in results.items():\n",
        "            f.write(f\"Question: {question}\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            if response:\n",
        "                answer_text = getattr(response, 'answer', str(response))\n",
        "                f.write(f\"Answer: {answer_text}\\n\")\n",
        "\n",
        "                contexts = getattr(response, 'contexts', getattr(response, 'context', []))\n",
        "                if contexts:\n",
        "                    f.write(f\"\\nSources ({len(contexts)}):\\n\")\n",
        "                    for i, ctx in enumerate(contexts, 1):\n",
        "                        ctx_name = getattr(ctx, 'name', getattr(ctx, 'doc', f'Source {i}'))\n",
        "                        f.write(f\"  {i}. {ctx_name}\\n\")\n",
        "            else:\n",
        "                f.write(\"Answer: No response available\\n\")\n",
        "            f.write(\"\\n\" + \"=\"*50 + \"\\n\\n\")\n",
        "\n",
        "    print(f\"💾 Results saved to: {filename}\")\n",
        "\n",
        "print(\"✅ Tutorial complete! You now have a fully functional PaperQA2 AI Agent with Gemini.\")"
      ]
    }
  ]
}